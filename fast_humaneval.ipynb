{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce713221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run to install requirements:\n",
    "# %pip install evalplus requests numpy tqdm transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1b3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Callable\n",
    "from evalplus.data import get_human_eval_plus, get_human_eval_plus_hash, write_jsonl\n",
    "from evalplus.evaluate import check_correctness, get_groundtruth\n",
    "from evalplus.sanitize import sanitize\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf5127",
   "metadata": {},
   "source": [
    "## API Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b7812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "\n",
    "def llamacpp_completion(text: str, wrap: bool = True, **kwargs) -> tuple[str, float]:\n",
    "    st = time.monotonic()\n",
    "    url = \"http://localhost:8080/completion\"\n",
    "    prompt = f\"\"\"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n{text}<|im_end|>\\n<|im_start|>assistant\\n\"\"\" if wrap else text\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": MAX_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    }\n",
    "    data.update(kwargs)\n",
    "    response = requests.post(url, json=data)\n",
    "    try:\n",
    "        completion = response.json()[\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"FAILURE:\", e, response.json())\n",
    "        raise e\n",
    "    dt = time.monotonic() - st\n",
    "    return completion, dt\n",
    "\n",
    "\n",
    "def vllm_completion(text: str, model: str, wrap: bool = True, **kwargs) -> tuple[str, float]:\n",
    "    st = time.monotonic()\n",
    "    token = \"YOUR_API_TOKEN\"\n",
    "    url = \"YOUR_URL_HERE/v1/completions\"\n",
    "    prompt = f\"\"\"<s> [INST] {text} [/INST] \"\"\" if wrap else text\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    }\n",
    "    data.update(kwargs)\n",
    "    response = requests.post(url, json=data, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    try:\n",
    "        completion = response.json()[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"FAILURE:\", e, response.json())\n",
    "        raise e\n",
    "    dt = time.monotonic() - st\n",
    "    return completion, dt\n",
    "\n",
    "\n",
    "def deepinfra_completion(text: str, model: str, wrap: bool = True, **kwargs) -> tuple[str, float]:\n",
    "    st = time.monotonic()\n",
    "    token = \"YOUR_API_TOKEN\"\n",
    "    url = f\"https://api.deepinfra.com/v1/inference/{model}\"\n",
    "    prompt = f\"\"\"<s> [INST] {text} [/INST] \"\"\" if wrap else text\n",
    "    data = {\n",
    "        \"input\": prompt,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    }\n",
    "    data.update(kwargs)\n",
    "    response = requests.post(url, json=data, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    try:\n",
    "        completion = response.json()[\"results\"][0][\"generated_text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"FAILURE:\", e, response.json())\n",
    "        raise e\n",
    "    dt = time.monotonic() - st\n",
    "    return completion, dt\n",
    "\n",
    "    \n",
    "def together_completion(text: str, model: str, wrap: bool = True, **kwargs) -> tuple[str, float]:\n",
    "    st = time.monotonic()\n",
    "    token = \"YOUR_API_TOKEN\"\n",
    "    url = \"https://api.together.xyz/v1/completions\"\n",
    "    prompt = f\"\"\"<s> [INST] {text} [/INST] \"\"\" if wrap else text\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    }\n",
    "    data.update(kwargs)\n",
    "    response = requests.post(url, json=data, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    try:\n",
    "        completion = response.json()[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"FAILURE:\", e, response.json())\n",
    "        raise e\n",
    "    dt = time.monotonic() - st\n",
    "    return completion, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ee4058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a simple implementation of the FizzBuzz problem in Python:\n",
      "\n",
      "```python\n",
      "for i in range(1, 101):\n",
      "    if i % 3 == 0 and i % 5 == 0:\n",
      "        print(\"FizzBuzz\")\n",
      "    elif i % 3 == 0:\n",
      "        print(\"Fizz\")\n",
      "    elif i % 5 == 0:\n",
      "        print(\"Buzz\")\n",
      "    else:\n",
      "        print(i)\n",
      "```\n",
      "\n",
      "This program will print out the numbers from 1 to 100, but for multiples of 3, it will print \"Fizz\" instead of the number, for multiples of 5, it will print \"Buzz\", and for multiples of both 3 and 5, it will print \"FizzBuzz\".\n"
     ]
    }
   ],
   "source": [
    "print(llamacpp_completion(\"Write a fizzbuzz in python\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dceec38",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_humaneval_instruction(language: str, question: str) -> str:\n",
    "    return '''You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n",
    "\n",
    "@@ Instruction\n",
    "Here is the given code to do completion:\n",
    "```{}\n",
    "{}\n",
    "```\n",
    "Please continue to complete the function with {} programming language. You are not allowed to modify the given code and do the completion only.\n",
    "\n",
    "Please return all completed codes in one code block.\n",
    "This code block should be in the following format:\n",
    "```{}\n",
    "# Your codes here\n",
    "```\n",
    "\n",
    "@@ Response\n",
    "'''.strip().format(language.lower(), question.strip(),language.lower(),language.lower())\n",
    "\n",
    "\n",
    "dataset = get_human_eval_plus()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"m-a-p/OpenCodeInterpreter-DS-6.7B\")\n",
    "print(\"{} examples for evaluation.\".format(len(dataset)))\n",
    "examples = []\n",
    "for task_id, example in tqdm(dataset.items(), desc='Generating'):\n",
    "    r = example.copy()\n",
    "    r[\"humaneval_prompt\"] = build_humaneval_instruction(\"python\", example['prompt'])\n",
    "    r[\"humaneval_inputs\"] = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': r[\"humaneval_prompt\"] }],\n",
    "        tokenize=False\n",
    "    )\n",
    "    examples.append(r)\n",
    "examples_by_task_id = {example[\"task_id\"]: example for example in examples}\n",
    "dataset_hash = get_human_eval_plus_hash()\n",
    "expected_output = get_groundtruth(dataset, dataset_hash, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f13d13",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0900ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_response(example: dict, plus: bool = False) -> int:\n",
    "    result = check_correctness(\n",
    "        \"humaneval\",\n",
    "        0,\n",
    "        example,\n",
    "        example[\"solution\"],\n",
    "        expected_output[example[\"task_id\"]],\n",
    "        base_only=not plus,\n",
    "        fast_check=False,\n",
    "    )\n",
    "    test_passes = result[\"plus\"] if plus else result[\"base\"]\n",
    "    ok = int(all(test_passes[1]))\n",
    "    return ok\n",
    "\n",
    "\n",
    "def collect_completions(\n",
    "    endpoint_callback: Callable, \n",
    "    model: str,\n",
    "    examples: list[dict], \n",
    "    workers: int = 64\n",
    ") -> list[tuple[str, str]]:\n",
    "    \n",
    "    def callback_wrapper(text: str, task_id: str) -> tuple[str, float, str]:\n",
    "        completion, dt = endpoint_callback(text, model=model)\n",
    "        return completion, dt, task_id\n",
    "    \n",
    "    st = time.monotonic()\n",
    "    completed = []\n",
    "    tokens = []\n",
    "    timings = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = []\n",
    "        for example in examples:\n",
    "            futures.append(executor.submit(\n",
    "                callback_wrapper, \n",
    "                text=example[\"humaneval_inputs\"], \n",
    "                task_id=example[\"task_id\"]\n",
    "            ))\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Collecting responses\"):\n",
    "            raw_response, rt, task_id = future.result()\n",
    "            completed.append((raw_response, task_id))\n",
    "            timings.append(rt)\n",
    "            tokens.append(tokenizer.encode(raw_response))\n",
    "            \n",
    "    dt = time.monotonic() - st\n",
    "    tok_count = sum([len(t) for t in tokens])\n",
    "    tm = np.mean(timings)\n",
    "    print(f'Requests: {len(examples)}\\tTime: {dt:0.2f} sec\\tSpeed {tok_count/dt:0.2f} tokens/sec\\tAvg. response time {tm:.2f} sec.')\n",
    "    return completed\n",
    "\n",
    "\n",
    "def measure_completions(completed: list[tuple[str, str]], name: str, plus: bool = False):\n",
    "    passes = []\n",
    "    solved = []\n",
    "    progress_bar = tqdm(completed, total=len(completed))\n",
    "    for raw_completion, task_id in progress_bar:\n",
    "        example = examples_by_task_id[task_id].copy()\n",
    "        example[\"raw_completion\"] = raw_completion\n",
    "        example[\"solution\"] = sanitize(raw_completion, entry_point=example[\"entry_point\"])\n",
    "        example[\"pass\"] = check_response(example, plus=plus)\n",
    "        passes.append(example[\"pass\"])\n",
    "        solved.append(example)\n",
    "        progress_bar.set_description(f\"Pass@1 {np.mean(passes):.3f}, fails {len(passes)-sum(passes)}\")\n",
    "\n",
    "    output_file = f\"humaneval_solutions_{name}.jsonl\"\n",
    "    write_jsonl(output_file, solved)\n",
    "    print(f\"Pass@1 {np.mean(passes):.3f}, fails {len(passes)-sum(passes)}\")\n",
    "    print(f\"Saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec8070",
   "metadata": {},
   "source": [
    "## Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f80f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting responses: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [01:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests: 164\tTime: 93.73 sec\tSpeed 550.97 tokens/sec\tAvg. response time 15.87 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.585, fails 68: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:18<00:00,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.585, fails 68\n",
      "Saved to humaneval_solutions_together_Mixtral-8x7B-Instruct-v0.1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval Together mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "completed = collect_completions(together_completion, \"mistralai/Mixtral-8x7B-Instruct-v0.1\", examples)\n",
    "measure_completions(completed, \"together_Mixtral-8x7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12e2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting responses: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:27<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests: 164\tTime: 27.05 sec\tSpeed 1852.20 tokens/sec\tAvg. response time 8.14 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.561, fails 72: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:19<00:00,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.561, fails 72\n",
      "Saved to humaneval_solutions_deepinfra_Mixtral-8x7B-Instruct-v0.1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval Deepinfra mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "completed = collect_completions(deepinfra_completion, \"mistralai/Mixtral-8x7B-Instruct-v0.1\", examples)\n",
    "measure_completions(completed, \"deepinfra_Mixtral-8x7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec7106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting responses: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:30<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests: 164\tTime: 30.92 sec\tSpeed 1469.15 tokens/sec\tAvg. response time 9.22 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.750, fails 41: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:18<00:00,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.750, fails 41\n",
      "Saved to humaneval_solutions_deepinfra_WizardLM-2-8x22B.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval Deepinfra microsoft/WizardLM-2-8x22B\n",
    "completed = collect_completions(deepinfra_completion, \"microsoft/WizardLM-2-8x22B\", examples)\n",
    "measure_completions(completed, \"deepinfra_WizardLM-2-8x22B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c6a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.695, fails 50: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [01:56<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.695, fails 50\n",
      "Saved to humaneval_solutions_deepinfra_WizardLM-2-8x22B_plus.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval-plus Deepinfra microsoft/WizardLM-2-8x22B\n",
    "measure_completions(completed, \"deepinfra_WizardLM-2-8x22B_plus\", plus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47a5b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting responses: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:32<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests: 164\tTime: 32.35 sec\tSpeed 1193.92 tokens/sec\tAvg. response time 9.87 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.756, fails 40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:19<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.756, fails 40\n",
      "Saved to humaneval_solutions_deepinfra_Mixtral-8x22B-Instruct-v0.1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval Deepinfra mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "completed = collect_completions(deepinfra_completion, \"mistralai/Mixtral-8x22B-Instruct-v0.1\", examples)\n",
    "measure_completions(completed, \"deepinfra_Mixtral-8x22B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6571dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.707, fails 48: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [01:22<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.707, fails 48\n",
      "Saved to humaneval_solutions_deepinfra_Mixtral-8x22B-Instruct-v0.1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval-plus Deepinfra mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "measure_completions(completed, \"deepinfra_Mixtral-8x22B-Instruct-v0.1\", plus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610aec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting responses: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [01:04<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests: 164\tTime: 64.24 sec\tSpeed 587.30 tokens/sec\tAvg. response time 20.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.750, fails 41: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:19<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.750, fails 41\n",
      "Saved to humaneval_solutions_together_Mixtral-8x22B-Instruct-v0.1.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Humaneval Together mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "completed = collect_completions(together_completion, \"mistralai/Mixtral-8x22B-Instruct-v0.1\", examples)\n",
    "measure_completions(completed, \"together_Mixtral-8x22B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f263d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.701, fails 49: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [01:23<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.701, fails 49\n",
      "Saved to humaneval_solutions_together_Mixtral-8x22B-Instruct-v0.1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Humaneval-plus Together mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "measure_completions(completed, \"together_Mixtral-8x22B-Instruct-v0.1\", plus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef038482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting responses: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [1:10:18<00:00, 25.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests: 164\tTime: 4218.70 sec\tSpeed 6.76 tokens/sec\tAvg. response time 25.72 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@1 0.805, fails 32: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:21<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 0.805, fails 32\n",
      "Saved to humaneval_solutions_llamacpp_codeqwen-1_5-7b-chat-q5_k_m.gguf.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Humaneval llama.cpp codeqwen-1_5-7b-chat-q5_k_m.gguf\n",
    "completed = collect_completions(llamacpp_completion, \"codeqwen-1_5-7b-chat-q5_k_m.gguf\", examples, workers=1)\n",
    "measure_completions(completed, \"llamacpp_codeqwen-1_5-7b-chat-q5_k_m.gguf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
